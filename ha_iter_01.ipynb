{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "# Pandas used for reading data and converting dataframes to machine understandable code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import Precision\n",
    "from pytorch_lightning.metrics import Recall\n",
    "from torchmetrics import (\n",
    "    Accuracy,\n",
    "    AUROC,\n",
    ")\n",
    "from typing import List, Dict\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    ")\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# from torch.utils.data import random_split\n",
    "\n",
    "# %load_ext nb_black\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# reading clinical data\n",
    "bc_df =pd.read_csv(\"data.csv\") # breast cancer DataFrame\n",
    "# loc = location\n",
    "# location of benign and malignant are seperated bc data is small and you want the data to be even\n",
    "bc_benign_diags = bc_df.loc[bc_df[\"diagnosis\"] == \"B\"]\n",
    "bc_malignant_diags = bc_df.loc[bc_df[\"diagnosis\"] == \"M\"]\n",
    "\n",
    "# print(type(bc_benign_diags[\"diagnosis\"]))\n",
    "\n",
    "# we dont need the id or unnamed coloumns from the dataFrame so drop is used to drop them\n",
    "# split up the data for two reasons: 1) Because there is not a lot of data we split to prevent the majority of the training set from being from one class. \n",
    "# 2) Double the Malignant to make the data equal\n",
    "# 3) Prevents false negatives. Even balance of both diagnosis\n",
    "# 4) Only want to double when necessary to avoid overfitting\n",
    "benign_examples = bc_benign_diags.drop([\"id\",\"Unnamed: 32\" ], axis = 1)\n",
    "# replaces B with 0.0 for binary classification\n",
    "benign_examples[\"diagnosis\"].replace(to_replace=\"B\", value=0.0, inplace=True)\n",
    "\n",
    "malignant_examples_orig = bc_malignant_diags.drop([\"id\",\"Unnamed: 32\" ], axis = 1)\n",
    "malignant_examples_orig[\"diagnosis\"].replace(to_replace=\"M\", value=1.0, inplace=True)\n",
    "\n",
    "# doubling the data\n",
    "malignant_examples = pd.concat([malignant_examples_orig, malignant_examples_orig])\n",
    "\n",
    "print(f\"malignant_examples size={len(malignant_examples)}\")\n",
    "print(f\"benign_examples size={len(benign_examples)}\")\n",
    "\n",
    "input_variable_names = [\n",
    "    i_labl for i_labl in benign_examples.columns if i_labl != \"diagnosis\"\n",
    "]\n",
    "\n",
    "# prints the first 5 example of each table\n",
    "malignant_examples.head()\n",
    "benign_examples.head()\n",
    "\n",
    "\n",
    "# print(f\"malignant_examples = {malignant_examples}\")\n",
    "# xxx = list(range(100))\n",
    "# dd = {\n",
    "#     \"x\": xxx,\n",
    "#     \"y\": [x * x + 2000 * random.random() - 1000 for x in xxx],\n",
    "# #     \"y\": [x * x for x in xxx],\n",
    "\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# examples_df = pd.DataFrame(data=dd)\n",
    "# examples_df.head()\n",
    "# print(examples_df.iloc[10, :])\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(examples_df.x, examples_df.y)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    \n",
    "    # This is the setup of the resnet\n",
    "    def __init__(self, num_inputs, layer_size, num_output, dropout_rate=0.0):\n",
    "\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        # TODO: discusiton point\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.lin_1 = nn.Linear(num_inputs, layer_size)\n",
    "        # torch.nn.init.xavier_uniform(self.lin_1.weight)\n",
    "        # torch.nn.init.xavier_uniform(self.lin_1.bias)\n",
    "        # self.lin_1.bias.data.fill_(0.1)\n",
    "        # m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.lin_2 = nn.Linear(layer_size, num_output)\n",
    "        # torch.nn.init.xavier_uniform(self.lin_2.weight)\n",
    "        # self.lin_2.bias.data.fill_(0.1)\n",
    "\n",
    "        # This is the resnet component that maps the dimension of the input to the output and adds them.\n",
    "        if num_inputs != num_output:\n",
    "            self.lin_map = nn.Linear(num_inputs, num_output, bias=False)\n",
    "            # torch.nn.init.xavier_uniform(self.lin_map.weight)\n",
    "            # self.lin_map.weight.data.fill_(1.0)\n",
    "\n",
    "        else:\n",
    "            self.lin_map = None\n",
    "\n",
    "    # override that defines how to forward propogate input through the defined layers of the model\n",
    "    \n",
    "    # set up geometry of the layers and set activation functions/dropout rate\n",
    "    # x refers to input\n",
    "    def forward(self, x):\n",
    "\n",
    "        nn_sp = nn.Softplus()\n",
    "\n",
    "        # TODO: discusiton point\n",
    "        nn_dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        output = self.lin_1(x)\n",
    "        output = nn_sp(output)\n",
    "        # output = nn_dropout(output)\n",
    "        output = self.lin_2(output)\n",
    "\n",
    "        if self.lin_map:\n",
    "            output = self.lin_map(x) + output\n",
    "        else:\n",
    "            output = x + output\n",
    "\n",
    "        # output = nn_sp(output)\n",
    "        return nn_dropout(output)\n",
    "\n",
    "\n",
    "# from pytorch-lightning\n",
    "class BreastCancerClassifier(pl.LightningModule):\n",
    "    # define model elements( e.i: layers and forward() function)\n",
    "    # TODO: remove hard coded num inputs and and layer size and num outputs\n",
    "    def __init__(self, hparams: Dict):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.val_accuracy = Accuracy(compute_on_step=False)\n",
    "        self.val_auroc = AUROC(compute_on_step=False)\n",
    "\n",
    "        self.train_accuracy = Accuracy(compute_on_step=False)\n",
    "        self.train_auroc = AUROC(compute_on_step=False)\n",
    "\n",
    "        self.temp_mod_lst = [\n",
    "            ResNetBlock(self.hparams.layer_size, self.hparams.layer_size, self.hparams.layer_size, self.hparams.dropout_rate) for i in range(self.hparams.num_layers)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ResNetBlock(self.hparams.num_inputs, self.hparams.layer_size, self.hparams.layer_size, self.hparams.dropout_rate),\n",
    "            *self.temp_mod_lst,\n",
    "            ResNetBlock(self.hparams.layer_size, self.hparams.layer_size, self.hparams.num_outputs),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # override that defines how to forward propogate input through the defined layers of the model\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        # print(f\"x = {x}, output={output}\")\n",
    "        # return F.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "    # ???\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer\n",
    "\n",
    "    # defines a loss function(mse) and an optimization algorithm\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        # print(f\"\\n training_step: train_batch = {train_batch}\\n\")\n",
    "        x, y = train_batch\n",
    "        # print(f\"training_step: x = {x}\")\n",
    "        # print(f\"training_step: y = {y}\")\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # print(f\"training_step: x = {x}\")\n",
    "        y_hat = self(x)\n",
    "        # print(f\"training_step: y_hat = {y_hat}\")\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        # print(f\"training_step: loss = {loss}\")\n",
    "\n",
    "        # print(f\"training_step: target = {y}, y_hat = {y_hat}, pred = {F.sigmoid(y_hat)}\")\n",
    "\n",
    "        self.log(\"train_loss\", loss, enable_graph=True, on_epoch=True, on_step=False)\n",
    "\n",
    "        # self.train_accuracy.update(F.sigmoid(y_hat), y.to(torch.int32))\n",
    "        self.train_accuracy.update(y_hat, y.to(torch.int32))\n",
    "\n",
    "        # self.train_auroc.update(F.sigmoid(y_hat), y.to(torch.int32))\n",
    "        self.train_auroc.update(y_hat, y.to(torch.int32))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, val_outputs):\n",
    "        try:\n",
    "            self.log(\"train_acc_epoch\", self.train_accuracy.compute(), on_epoch=True)\n",
    "            self.train_accuracy.reset()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.log(\"train_auroc_epoch\", self.train_auroc.compute())\n",
    "            self.train_auroc.reset()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        #         print(f\"val_batch = {val_batch}\")\n",
    "\n",
    "        x, y = val_batch\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        y_hat = self.model(x)\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, enable_graph=True)\n",
    "\n",
    "        # print(f\"validation_step: target = {y}, y_hat = {y_hat}, pred = {F.sigmoid(y_hat)}\")\n",
    "        # self.val_accuracy.update(F.sigmoid(y_hat), y.to(torch.int32))\n",
    "        self.val_accuracy.update(y_hat, y.to(torch.int32))\n",
    "\n",
    "        # self.val_auroc.update(F.sigmoid(y_hat), y.to(torch.int32))\n",
    "        self.val_auroc.update(y_hat, y.to(torch.int32))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, val_outputs):\n",
    "        try:\n",
    "            self.log(\"valid_acc_epoch\", self.val_accuracy.compute(), on_epoch=True)\n",
    "            self.val_accuracy.reset()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.log(\"valid_auroc_epoch\", self.val_auroc.compute())\n",
    "            self.val_auroc.reset()\n",
    "        except:\n",
    "            pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ExamplesDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, input_variable_names: list):\n",
    "        #         self.data = torch.FloatTensor(data.values.astype(\"float\"))\n",
    "        self.data = data\n",
    "        self.input_variable_names = input_variable_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_item = self.data.iloc[index, :].astype(\"float32\")\n",
    "\n",
    "        input_values = [one_item[a_label] for a_label in self.input_variable_names]\n",
    "        target_value = one_item[\"diagnosis\"]\n",
    "\n",
    "        # print(f\"ExamplesDataset: input vals = {input_values}, target = {target_value}\")\n",
    "        return torch.tensor(input_values), torch.tensor([target_value])\n",
    "\n",
    "\n",
    "class BreastCancerClassifierDataLoader(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, benign_examples: pd.DataFrame, malignant_examples: pd.DataFrame, input_variable_names: List[str],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # print(f\"benign examples = {benign_examples}\")\n",
    "\n",
    "        self.batch_size = 1\n",
    "        self.benign_examples = benign_examples\n",
    "        self.malignant_examples = malignant_examples\n",
    "\n",
    "        self.input_variable_names = input_variable_names\n",
    "\n",
    "        # print(f\"input labels = {self.input_variable_names}\")\n",
    "\n",
    "        # Setting the training data\n",
    "        self.benign_train_df = self.benign_examples.sample(frac=0.8)\n",
    "        self.malignant_train_df = self.malignant_examples.sample(frac=0.8)\n",
    "        # Concatenating both training sets\n",
    "        self.train_df = pd.concat([self.benign_train_df, self.malignant_train_df])\n",
    "        # print(f\"train_df = {self.train_df}\")\n",
    "        self.train_ds = ExamplesDataset(self.train_df, self.input_variable_names)\n",
    "\n",
    "        self.benign_val_df = self.benign_examples.drop(self.benign_train_df.index)\n",
    "        self.malignant_val_df = self.malignant_examples.drop(\n",
    "            self.malignant_train_df.index\n",
    "        )\n",
    "        self.val_df = pd.concat([self.benign_val_df, self.malignant_val_df])\n",
    "        # print(f\"val_df = {self.val_df}\")\n",
    "        self.val_ds = ExamplesDataset(self.val_df, self.input_variable_names)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_df, batch_size=2)\n",
    "# val_loader = DataLoader(val_df, batch_size=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dm_loader = BreastCancerClassifierDataLoader(\n",
    "    benign_examples, malignant_examples, input_variable_names\n",
    ")\n",
    "\n",
    "# model\n",
    "hparams = {\n",
    "    \"num_inputs\": 30,\n",
    "    \"num_outputs\": 1,\n",
    "    \"layer_size\": 34,\n",
    "    \"dropout_rate\": 0.04,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_layers\": 8,\n",
    "}\n",
    "\n",
    "model = BreastCancerClassifier(hparams)\n",
    "\n",
    "# training\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=600, logger=True, progress_bar_refresh_rate=50)\n",
    "start_time = time.time()\n",
    "trainer.fit(model, dm_loader)\n",
    "print(f\"training duration: {time.time()-start_time} seconds\")\n",
    "# trainer.fit(model, dm_loader.train_dataloader(), dm_loader.val_dataloader())\n",
    "# print(model)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# xxxt = torch.tensor(range(100), dtype=torch.float32)\n",
    "# xxxt = torch.unsqueeze(xxxt, 1)\n",
    "# # print(xxxt)\n",
    "# yyyp = model(xxxt)\n",
    "# # print(yyyp)\n",
    "# # plt.plot(xxxt.detach().numpy(), yyyp.detach().numpy())\n",
    "\n",
    "trained_model = BreastCancerClassifier.load_from_checkpoint(\"./lightning_logs/version_6/checkpoints/epoch=168-step=105624.ckpt\")\n",
    "\n",
    "# TODO: discussion point\n",
    "trained_model.eval()\n",
    "\n",
    "print(f\"model params: {trained_model.hparams}\")\n",
    "\n",
    "for idx, mlex in pd.concat([malignant_examples, benign_examples]).iterrows():\n",
    "    input_values = [mlex[a_label] for a_label in input_variable_names]\n",
    "    target_value = mlex[\"diagnosis\"]\n",
    "\n",
    "    target_tnsr = torch.tensor([target_value], dtype=torch.float32)\n",
    "    input_tnsr = torch.tensor([input_values], dtype=torch.float32)\n",
    "    # print(input_tnsr)\n",
    "    print(f\"model prediction: {trained_model(input_tnsr)}, actual diagnosis: {target_value}\" )\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67d6ba6b43a7f1e97016f6197b2671f68283074389a1722bd6c94e93d00763ec"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('ml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}